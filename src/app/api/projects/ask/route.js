// src/app/api/projects/ask/route.js
import { NextResponse } from "next/server";
import { getServerSession } from "next-auth";
import { prisma } from "@/lib/prisma";
import OpenAI from "openai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

/**
 * Normalizes text for matching file names / user question
 */
function normalize(str = "") {
  return String(str || "")
    .toLowerCase()
    .normalize("NFKD") // fold accents
    .replace(/[\u0300-\u036f]/g, "") // remove diacritics
    .replace(/[^a-z0-9\s.-]/g, " ") // keep alphanum, dot, dash
    .replace(/\s+/g, " ")
    .trim();
}


const k1 = 1.5;
const b = 0.75;

function tokenize(text) {
  return normalize(text).split(" ").filter(w => w.length > 2);
}

function computeBM25(chunks, query) {
  const queryTokens = tokenize(query);
  const N = chunks.length;

  // Precompute avg document length
  const avgdl = chunks.reduce((s, c) => s + tokenize(c.text || "").length, 0) / N;

  // Precompute document frequencies DF(term)
  const df = {};
  for (const t of queryTokens) {
    df[t] = chunks.filter(c => tokenize(c.text || "").includes(t)).length || 0;
  }

  // IDF(term)
  const idf = {};
  for (const t of queryTokens) {
    const df_t = df[t];
    idf[t] = Math.log( (N - df_t + 0.5) / (df_t + 0.5) + 1 );
  }

  // BM25 score for each chunk
  const scores = chunks.map(chunk => {
    const tokens = tokenize(chunk.text || "");
    const dl = tokens.length;
    let score = 0;

    for (const t of queryTokens) {
      const tf = tokens.filter(x => x === t).length;
      if (tf === 0) continue;

      const denom = tf + k1 * (1 - b + b * (dl / avgdl));
      score += idf[t] * ((tf * (k1 + 1)) / denom);
    }

    return { ...chunk, score };
  });

  return scores.sort((a, b) => b.score - a.score);
}

export async function POST(req) {
  try {
    const session = await getServerSession();
    if (!session?.user?.email)
      return NextResponse.json({ error: "Unauthorized" }, { status: 401 });

    const body = await req.json();
    const { projectId, question } = body || {};

    if (!projectId || !question)
      return NextResponse.json({ error: "Missing projectId or question" }, { status: 400 });

    // verify project belongs to user
    const project = await prisma.project.findFirst({
      where: { id: projectId, user: { email: session.user.email } },
    });
    if (!project) return NextResponse.json({ error: "Project not found" }, { status: 404 });

    console.log("üí¨ Project ask:", projectId, question);

    // 1) Conversation (latest or new)
    let conv = await prisma.projectConversation.findFirst({
      where: { projectId },
      orderBy: { createdAt: "desc" },
    });

    if (!conv) {
      const user = await prisma.user.findUnique({
        where: { email: session.user.email },
        select: { id: true },
      });

      conv = await prisma.projectConversation.create({
        data: {
          projectId,
          userId: user.id,
        },
      });
    }

    // 2) Insert user message (pending)
    const userMsg = await prisma.projectMessage.create({
      data: {
        conversationId: conv.id,
        role: "user",
        content: question,
        status: "pending",
      },
    });

    // üü° Debug: selected vs unselected docs for mention detection
    const selectedDocsRaw = await prisma.document.findMany({
      where: { projectId, selected: 1 },
      select: { id: true, filename: true },
    });

    const unselectedDocsRaw = await prisma.document.findMany({
      where: { projectId, selected: 0 },
      select: { id: true, filename: true },
    });

    console.log("üìÑ Selected documents:", selectedDocsRaw);
    console.log("‚ùå Unselected documents:", unselectedDocsRaw);

    const q = normalize(question);

    // 1Ô∏è‚É£ Case: User explicitly says "unselected document"
    if (q.includes("unselected document")) {
      // mark user message done
      await prisma.projectMessage.update({
        where: { id: userMsg.id },
        data: { status: "done" },
      });
      return NextResponse.json({
        success: true,
        answer: "The document is unselected or does not exist, so I cannot answer that.",
      });
    }

    // 2Ô∏è‚É£ Extract names of unselected + selected docs mentioned in the question
    const mentionedUnselected = [];
    const mentionedSelected = [];

    for (const ud of unselectedDocsRaw) {
      if (q.includes(normalize(ud.filename))) {
        mentionedUnselected.push(ud.filename);
      }
    }

    for (const sd of selectedDocsRaw) {
      if (q.includes(normalize(sd.filename))) {
        mentionedSelected.push(sd.filename);
      }
    }

    // 3Ô∏è‚É£ If user mentions ONLY unselected docs ‚Üí block
    if (mentionedUnselected.length > 0 && mentionedSelected.length === 0) {
      // mark user message done
      await prisma.projectMessage.update({
        where: { id: userMsg.id },
        data: { status: "done" },
      });

      if (mentionedUnselected.length === 1) {
        return NextResponse.json({
          success: true,
          answer: `The document "${mentionedUnselected[0]}" is unselected, so I cannot answer that.`,
        });
      } else {
        return NextResponse.json({
          success: true,
          answer: "The document is unselected or does not exist, so I cannot answer that.",
        });
      }
    }

    // 4) Load selected documents only (enforce selected filter)
    const docs = await prisma.document.findMany({
      where: { projectId, selected: 1 },
    });

    if (!docs.length) {
      await prisma.projectMessage.update({
        where: { id: userMsg.id },
        data: { status: "done" },
      });
      return NextResponse.json({
        success: true,
        answer: "No documents found in this project.",
      });
    }

    // 5) Load all chunks for selected documents
    const docIds = docs.map((d) => d.id);
    const chunks = await prisma.chunk.findMany({
      where: { documentId: { in: docIds } },
    });

    const allChunks = chunks.map((c) => {
      const doc = docs.find((d) => d.id === c.documentId);
      return {
        id: c.id,
        documentId: c.documentId,
        documentName: doc?.filename || "untitled",
        chunkIndex: c.chunkIndex,
        text:  c.text || c.summary || "",
        docSummary: doc?.summary || "",
        embedding: c.embedding || null,
      };
    });

    // 6) Missing embeddings


    const docStatuses = docs.map(d => d.status);

    const allChunkedOrExtracting = docStatuses.every(s =>
      s === "chunked" || s === "extracting" || s === "pending"
    );

    const hasEmbeddingsReady = docStatuses.some(s =>
      s === "embedded" || s === "summarizing" || s === "ready"
    );

    console.log("üìä Document statuses:", docStatuses);

    if (allChunkedOrExtracting && !hasEmbeddingsReady) {
      console.log("üü¶ BASIC CHAT (BM25): embeddings not ready");

      // BM25 scoring
      const scored = computeBM25(allChunks, question).slice(0, 8);

      const contextBlocks = scored
        .map(s => `Document: ${s.documentName}\n- ${s.text.slice(0, 600).replace(/\n+/g, " ")}`)
        .join("\n\n");

      const systemMsg = {
        role: "system",
        content: `
You are answering based on extracted text only.
Embeddings are not ready yet.
Use ONLY the provided text. Do not invent facts.
BM25 retrieval has selected the most relevant chunks.
        `.trim(),
      };

      // Load previous messages
      const prevMsgs = await prisma.projectMessage.findMany({
        where: { conversationId: conv.id, createdAt: { lt: userMsg.createdAt } },
        orderBy: { createdAt: "desc" },
        take: 6,
      });
      prevMsgs.reverse();

      const messages = [
        systemMsg,
        ...prevMsgs.map(m => ({ role: m.role, content: m.content })),
        {
          role: "user",
          content: `Question: ${question}\n\nContext:\n${contextBlocks}`,
        },
      ];

      console.log("üü¶ Sending BM25 mode request to GPT");

      const completion = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages,
        temperature: 0.3,
        max_tokens: 800,
      });

      const assistantText = completion?.choices?.[0]?.message?.content?.trim() || "";

      // Store user & assistant messages
      await prisma.projectMessage.update({
        where: { id: userMsg.id },
        data: { status: "done" },
      });

      await prisma.projectMessage.create({
        data: {
          conversationId: conv.id,
          role: "assistant",
          content: assistantText,
          status: "done",
        },
      });

      return NextResponse.json({ success: true, answer: assistantText });
    }



    const missing = allChunks.filter(c => !c.embedding);

    // 7) Inline embed small missing counts (‚â§10)
    if (missing.length > 0) {
      for (const chunk of missing) {
        const embRes = await openai.embeddings.create({
          model: "text-embedding-3-small",
          input: String(chunk.text).slice(0, 8000),
        });
        const emb = embRes.data[0].embedding;
        await prisma.chunk.update({
          where: { id: chunk.id },
          data: { embedding: emb },
        });
        chunk.embedding = emb;
      }
    }

    // 8) Embed question
    const qEmb = await openai.embeddings.create({
      model: "text-embedding-3-small",
      input: question,
    });
    const queryEmbedding = qEmb.data[0].embedding;

    // 9) Similarity (cosine)
    const cosineSim = (a, b) => {
      const dot = a.reduce((s, x, i) => s + x * b[i], 0);
      const normA = Math.sqrt(a.reduce((s, x) => s + x * x, 0));
      const normB = Math.sqrt(b.reduce((s, x) => s + x * x, 0));
      if (normA === 0 || normB === 0) return 0;
      return dot / (normA * normB);
    };

    const scored = allChunks
      .filter((c) => c.embedding)
      .map((c) => ({ ...c, score: cosineSim(queryEmbedding, c.embedding) }))
      .sort((a, b) => b.score - a.score);

    const selected = scored.slice(0, Math.min(8, scored.length));

    // 10) Build context grouped by document
    const grouped = selected.reduce((acc, s) => {
      acc[s.documentName] = acc[s.documentName] || [];
      acc[s.documentName].push(s);
      return acc;
    }, {});

    const contextBlocks = Object.entries(grouped).map(([docName, chunks]) => {
      const first = chunks[0];
      const docSummary = first.docSummary ? `Summary: ${first.docSummary}` : "";
      const chunkTexts = chunks
        .map((s) => `- (Chunk ${s.chunkIndex}) ${String(s.text).slice(0, 600).replace(/\n+/g, " ")}`)
        .join("\n");
      return `Document: ${docName}\n${docSummary}\n${chunkTexts}`;
    });

    const docMeta = docs.map((d, i) => `${i + 1}. ${d.filename}`).join("\n");
    const context = `Project contains ${docs.length} documents:\n${docMeta}\n\n${contextBlocks.join("\n\n")}`;

    // 11) Short-term memory: previous messages (createdAt < userMsg.createdAt)
    const prevMsgs = await prisma.projectMessage.findMany({
      where: { conversationId: conv.id, createdAt: { lt: userMsg.createdAt } },
      orderBy: { createdAt: "desc" },
      take: 6,
    });
    prevMsgs.reverse();

    const systemMsg = {
      role: "system",
      content: `
You are an expert assistant that answers questions *based on selected project documents*.

You may:
- Summarize document content
- Explain document content
- Compare document content
- Generate new text (letters, emails, reports, etc.)
  as long as the factual information used comes from the selected documents.

Do NOT:
- Use information from unselected documents.
- Invent factual information that is not supported by the selected documents.

If a user asks about an unselected document:
"The document is unselected or does not exist, so I cannot answer that."

If a factual answer cannot be found in the selected documents:
"I cannot answer that based on the selected documents."

Response format:
- Plain text only (no markdown, no lists, no special formatting).
      `.trim(),
    };

    const memoryMsgs = prevMsgs.map((m) => ({ role: m.role, content: m.content }));
    const userMsgForModel = { role: "user", content: `Question: ${question}\n\nContext:\n${context}` };

    // 12) GPT call
    const completion = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [systemMsg, ...memoryMsgs, userMsgForModel],
      temperature: 0.3,
      max_tokens: 800,
    });

    const assistantText = (completion?.choices?.[0]?.message?.content || "")
      .replace(/\*/g, "")
      .trim();

    // 13) Persist messages: update user -> done and insert assistant
    await prisma.projectMessage.update({
      where: { id: userMsg.id },
      data: { status: "done" },
    });

    await prisma.projectMessage.create({
      data: {
        conversationId: conv.id,
        role: "assistant",
        content: assistantText,
        status: "done",
      },
    });

    return NextResponse.json({ success: true, answer: assistantText });
  } catch (err) {
    console.error("‚ùå ask-project:", err);
    return NextResponse.json({ success: false, error: err.message }, { status: 500 });
  }
}
